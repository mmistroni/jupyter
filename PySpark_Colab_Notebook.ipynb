{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark Colab Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/jupyter/blob/master/PySpark_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRJN0dtZbgz3",
        "colab_type": "text"
      },
      "source": [
        "<h3> Installing dependencies</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRWUxU4Lblvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34P4-gJmb86W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5zynArKcC90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, LongType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPbxToH2QZcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QWU5g5fcK_P",
        "colab_type": "text"
      },
      "source": [
        "<p> ToDO test few runs here, then  store a dataframe of all possible shares for last 2 years and run a correlation between all cols </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh_f3x8Pn5Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_df = \\\n",
        "\"\"\"dept,cluster,usd_amt,cluster_amt\n",
        "D1,,10,50\n",
        "D2,C2,20,100\n",
        "D3,C2,70,100\n",
        "D1,C2,10,100\n",
        "D5,C1,40,50\n",
        "D3,C3,70,120\n",
        "D4,C3,50,120\"\"\"\n",
        "data = simple_df.split('\\n')\n",
        "rows = [ln.split(',') for ln in data]\n",
        "headers = rows[0]\n",
        "vals = rows[1:] \n",
        "rdd = spark.sparkContext.parallelize(vals)\n",
        "schema = StructType([StructField(fname, StringType(), True) for fname in headers])\n",
        "df = spark.createDataFrame(rdd, schema)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUuqz-GzRERl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.orderBy(*['dept', 'cluster']).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqd-3l6ER7XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt = df.groupby('dept').agg(F.sum('cluster_amt').alias('total_clust_amt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5IXxHAiT6Ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMseYlYfSeXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jnd = tt.join(df, ['dept'])\n",
        "no_func_res = jnd.withColumn('dept_pcnt', F.col('cluster_amt')/F.col('total_clust_amt')).orderBy(*['dept','cluster'])\n",
        "no_func_res.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of31CJ8ace1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEstign window function on df\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izmagS5Gwm_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_df = \\\n",
        "\"\"\"le,cpty,cpty_cs_or_le\n",
        "CSI,First,3rd Bank\n",
        "CSI,First,3rd Investment\n",
        "CSI,First,3rd Bank\n",
        "CSI,First,3rd PincoPallino\n",
        "CSI,First,3rd Bank\n",
        "CSI,First,3rd Bank\"\"\"\n",
        "data = simple_df.split('\\n')\n",
        "rows = [ln.split(',') for ln in data]\n",
        "headers = rows[0]\n",
        "vals = rows[1:] \n",
        "rdd = spark.sparkContext.parallelize(vals)\n",
        "schema = StructType([StructField(fname, StringType(), True) for fname in headers])\n",
        "df = spark.createDataFrame(rdd, schema)\n",
        "df.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X7oovV8cexE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX6Ib4h1cent",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "w2 = Window.partitionBy('le', 'cpty')\n",
        "res = df.withColumn('tparty_cs_or_le',F.collect_set('cpty_cs_or_le').over(w2))\n",
        "#concat_ws(',', split(df.emailed))\n",
        "res = res.withColumn('concatenated_le', F.concat_ws(',', 'tparty_cs_or_le'))\n",
        "res.select('concatenated_le').collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvMlRQ9tdtkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res.schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVshNskkWSYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empsalary = [\n",
        "  [\"sales\",     1,  \"Alice\",  5000],\n",
        "  [\"personnel\", 2,  \"Olivia\", 3900],\n",
        "  [\"sales\",     3,  \"Ella\",   4800],\n",
        "  [\"sales\",     4,  \"Ebba\",   4800],\n",
        "  [\"personnel\", 5,  \"Lilly\",  3500],\n",
        "  [\"develop\",   7,  \"Astrid\", 4200],\n",
        "  [\"develop\",   8,  \"Saga\",   6000],\n",
        "  [\"develop\",   9,  \"Freja\",  4500],\n",
        "  [\"develop\",   10, \"Wilma\",  5200],\n",
        "  [\"develop\",   11, \"Maja\",   5200]]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(empsalary)\n",
        "schema = StructType([StructField('depName', StringType(), True),\n",
        "                     StructField('empNo', LongType(), True),\n",
        "                     StructField('name', StringType(), True) ,\n",
        "                     StructField('salary', LongType(), True)])\n",
        "empsalary_df = spark.createDataFrame(rdd, schema)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnEN72wAZJ1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "w = Window.partitionBy('depName')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swdzmpPyaTqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empsalary_df.withColumn('total_rank',F.sum('salary').over(w)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Tz6kWGn5I_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def calculate_month_diffs(maxdate, current):\n",
        "  pass\n",
        "\n",
        "\n",
        "with_dt = df.withColumn('cobdate_str', F.concat(F.col('year'),F.lit('-'), F.col('month'))) # this is the first dataframe\n",
        "print('with dt has:{}'.format(with_dt.count()))\n",
        "with_cobdt = with_dt.select('cobdate_str').distinct().withColumn('cobdate', F.to_date(F.col('cobdate_str'))).distinct()\n",
        "print('Wtih cobdt has:{}'.format(with_cobdt.count()))\n",
        "\n",
        "max_dt = with_cobdt.select(F.max(F.col('cobdate')).alias('max_date')).collect()[0]['max_date']\n",
        "with_mx = with_cobdt.withColumn('max_dt', F.lit(max_dt))\n",
        "res = with_mx.withColumn('month_diffs', F.months_between(F.col('max_dt'), F.col('cobdate')).cast(IntegerType()))\\\n",
        "            .filter(F.col('month_diffs') <= 12)\n",
        "\n",
        "joined = with_dt.join(res, ['cobdate_str'])\n",
        "\n",
        "joined.toPandas()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heg-gxb8SF7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "from dateutil import relativedelta\n",
        "\n",
        "##Aug 7 1989 8:10 pm\n",
        "date_1 = datetime(1989, 8, 1, 20, 10)\n",
        "\n",
        "##Dec 5 1990 5:20 am\n",
        "date_2 = datetime(1990, 12, 5, 5, 20)\n",
        "\n",
        "#This will find the difference between the two dates\n",
        "difference = relativedelta.relativedelta(date_2, date_1)\n",
        "\n",
        "years = difference.years\n",
        "months = difference.months\n",
        "days = difference.days\n",
        "hours = difference.hours\n",
        "minutes = difference.minutes\n",
        "\n",
        "print (\"Difference is %s year, %s months, %s days, %s hours, %s minutes \" %(years, months, days, hours, minutes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f1PUnH-n5Ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.toPandas()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxVPfvm3n5DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L18obK1Gn4_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t9MCtHrn46z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ24WhRrcGV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this link to calculate correlation https://stackoverflow.com/questions/45112976/how-to-use-correlation-in-spark-with-dataframes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IATQYipBerJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas-datareader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX6pv8n_XEAV",
        "colab_type": "text"
      },
      "source": [
        "<h3>Getting IEXApi Token </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HevGu1mIXHQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_iexapi_keys():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  with open('gdrive/My Drive/passwords/iexapi.keys') as f:\n",
        "    return f.readlines()[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igiM3aVKx70z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pandas_datareader.data as dr\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "\n",
        "iexapi_token = get_iexapi_keys()\n",
        "\n",
        "\n",
        "def get_historical_value(symbol):\n",
        "  try: \n",
        "    data = dr.get_data_yahoo(symbol, date(2018,1,1), date(2019,9,19))[['Adj Close']]\n",
        "    return data.rename(columns={'Adj Close' : symbol})\n",
        "    \n",
        "  except Exception as e :\n",
        "    print('Exception for {}={}'.format(symbol, str(e)))\n",
        "    return pd.DataFrame(columns=[symbol])\n",
        "  \n",
        "def get_all_symbols():\n",
        "  all_symbols_data = requests.get('https://cloud.iexapis.com/stable/ref-data/iex/symbols?token={}'.format(iexapi_token)).json()\n",
        "  good_ones = [d['symbol'] for d in all_symbols_data if d['isEnabled']]\n",
        "  return filter(lambda data: bool(data), good_ones)\n",
        "\n",
        "def get_all_etfs():\n",
        "  nyse_symbols = requests.get('https://cloud.iexapis.com/stable/ref-data/exchange/nys/symbols?token={}'.format(iexapi_token)).json()\n",
        "  nas_symbols = requests.get('https://cloud.iexapis.com/stable/ref-data/exchange/nas/symbols?token={}'.format(iexapi_token)).json()\n",
        "  return [d['symbol'] for d in nyse_symbols + nas_symbols if d['type'].lower() == 'et']\n",
        "\n",
        "def get_all_stocks_data():\n",
        "  all_symbols_data =  get_all_etfs()\n",
        "  return map(lambda symbol: get_historical_value(symbol), all_symbols_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWphjX7eX0WZ",
        "colab_type": "text"
      },
      "source": [
        "<h3> Comparing shares correlation with VIX via Pandas and Spark </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnaolPCRyWrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vix = get_historical_value('^VIX')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL7HqxzP-dZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "all_stocks  = get_all_stocks_data()\n",
        "\n",
        "res= [vals for vals in all_stocks if vals.shape[0] == vix.shape[0]]\n",
        "res.append(vix)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnGqWPHr9z-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building pandas df\n",
        "all_data = pd.concat(res, axis=1)\n",
        "spark_df = spark.createDataFrame(all_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CcinG0yyiUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_correlation(df):\n",
        "  from pyspark.ml.stat import Correlation\n",
        "  from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "  # convert to vector column first\n",
        "  vector_col = \"corr_features\"\n",
        "  assembler = VectorAssembler(inputCols=df.columns, outputCol=vector_col)\n",
        "  df_vector = assembler.transform(df).select(vector_col)\n",
        "\n",
        "  # get correlation matrix\n",
        "  matrix = Correlation.corr(df_vector, vector_col)\n",
        "  return matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8XLi0FFymx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = spark_df.columns\n",
        "matrix = calculate_correlation(spark_df)\n",
        "matrix.toPandas()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvdOxxxLzEAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to numpy\n",
        "np_array = matrix.collect()[0][\"pearson({})\".format(\"corr_features\")].toArray()\n",
        "np_array.shape\n",
        "\n",
        "vix_row = np_array[-1:].tolist()[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5o8T42pKb1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipped = zip(columns, vix_row)\n",
        "\n",
        "sorted_zip = sorted(zipped, key=lambda tpl:tpl[1], reverse=True)\n",
        "from pprint import pprint\n",
        "pprint(list(sorted_zip)[0:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g45i6smez4_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "r1 = Correlation.corr(df, \"features\").head()\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhR1VTOt0uat",
        "colab_type": "text"
      },
      "source": [
        "<h3> Alternative approach via RDD </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cjtiCOS1ErF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_historical_value_df(symbol):\n",
        "  try: \n",
        "    data = dr.get_data_yahoo(symbol, date(2018,1,1), date(2019,9,19))[['Adj Close']]\n",
        "    pandas_df = data.rename(columns={'Adj Close' : 'adj_close'})['adj_close']\n",
        "    return spark.createDataFrame(pandas_df, FloatType())\n",
        "    \n",
        "  except Exception as e :\n",
        "    print('Excepiton for {}:{}'.format(symbol, str(e)))\n",
        "    return []\n",
        "\n",
        "def calculate_correlation(df1, df2, field1, field2)\n",
        "  firstRDD = df1.select(\"adj_close\").map(lambda r: row.getDouble(0))\n",
        "  val secondRDD: RDD[Double] = yourDF.select(\"field2\").map(row => row.getDouble(0))\n",
        "  val corr = Statistics.corr(firstRDD, secondRDD, \"spearman\")  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nAsl25T1Rhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vix = get_historical_value_df('^VIX')\n",
        "\n",
        "vals = [x for x in map(lambda r: r.asDict(), vix)]\n",
        "from pprint import pprint\n",
        "vals\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzYf7ea70SQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mkagTa1Ry4L",
        "colab_type": "text"
      },
      "source": [
        "<h3> Testing date selection </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZDsdFXXR2Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranges = list(range(0,13))\n",
        "list_2018 = [2018] * 13\n",
        "zipped1 = list(zip(list_2018, ranges))\n",
        "zipped2 = list(zip([2019] * 13, ranges))\n",
        "zipped3 = list(zip([2020] * 1, ranges))\n",
        "\n",
        "zipped1 + zipped2 + zipped3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzX3feQcR9Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import date\n",
        "import datetime\n",
        "import pandas as pd\n",
        "def create_filter(current_dt):\n",
        "  # we take everything for current year\n",
        "  current_month = current_dt.month\n",
        "  current_year = current_dt.year\n",
        "  print('Current month:{}, Current year:{}'.format(current_month, current_year))\n",
        "  curent_yr_filter = (F.col('year') == current_year) & (F.col('month') <= current_month)\n",
        "  print('currnet year filter is:{}'.format(curent_yr_filter)) \n",
        "  last_yr_start = (pd.to_datetime(current_dt) - pd.DateOffset(months=12))\n",
        "  print('las tyer starting from:{}'.format(last_yr_start))\n",
        "  last_yr_month = last_yr_start.month\n",
        "  print('Including zeros..')\n",
        "  real_month = last_yr_month if last_yr_month > 1 else 0\n",
        "  last_yr_filter = (F.col('year') == current_year-1) & (F.col('month') >= real_month)\n",
        "  print('Last year filter:{}'.format(last_yr_filter))\n",
        "  \n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLRarLT0TD0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dt in [date(2020,1,1),\n",
        "           date(2019,12,31),\n",
        "           date(2019,11,30),\n",
        "           date(2019,10,31)]:\n",
        "  print('TEsting:{}'.format(dt))\n",
        "  create_filter(dt)\n",
        "  print('-----')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM_TsvU_TGMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "(pd.to_datetime(test_dt) - pd.DateOffset(months=12)).month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgf9uyCzUvmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}