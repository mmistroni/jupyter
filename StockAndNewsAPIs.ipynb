{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockAndNewsAPIs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/jupyter/blob/master/StockAndNewsAPIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9xnZo-xamUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install pandas-datareader\n",
        "!pip install nltk\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giosOkpiMPyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEJyyYSJaraF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import BDay\n",
        "import pandas_datareader.data as dr\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen, Request\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtzWiZO4XBri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as  pd\n",
        "from datetime import date\n",
        "import logging\n",
        "from pandas.tseries.offsets import BDay\n",
        "import requests\n",
        "\n",
        "def get_data(ticker, dt, busdays=1):\n",
        "  today = date.today()\n",
        "  start_date = (today- BDay(busdays))\n",
        "  full_url = 'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?from={start_date}&to={end_date}'\\\n",
        "                    .format(ticker=ticker, start_date=start_date.strftime('%Y-%m-%d'), end_date=today.strftime('%Y-%m-%d'))\n",
        "  return requests.get(full_url).json()\n",
        "\n",
        "def get_latest_price_yahoo_dataflow(ticker, bday=1):\n",
        "  from datetime import date\n",
        "  today = date.today().strftime('%Y-%m-%d')\n",
        "  start_date = today- BDay(bday)\n",
        "  today_df = get_data(ticker, today)\n",
        "  yday_df = get_data(ticker, start_date).strftime('%Y-%m-%d')\n",
        "  yday_df = yday_df.rename(columns={\"Adj Close\": \"Prev Close\", \"Volume\": \"Prev Volume\"})\n",
        "  merged = pd.merge(today_df, yday_df, on='Symbol')\n",
        "  merged['Diff'] = merged['Adj Close'] - merged['Prev Close']\n",
        "  merged['Vol Diff'] = merged['Volume'] - merged['Prev Volume']\n",
        "  return merged\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqrFVyKPlrdX",
        "colab_type": "text"
      },
      "source": [
        "<h2>Authenticate User </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXM3PKNDlvaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2XGV6H1qGF6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<h3>Loading Credentials</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU0JmFUBNRkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def get_iexapi_keys():\n",
        "  with open('gdrive/My Drive/passwords/iexapi.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "def get_nlp_service_keys():\n",
        "  with open('gdrive/My Drive/passwords/nlp.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "def get_newsapi_keys():\n",
        "  with open('gdrive/My Drive/passwords/newsapi.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "def getfmpkeys():\n",
        "  with open('gdrive/My Drive/passwords/fmprep.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7zA8M_Owwq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prices(ticker):\n",
        "    iexapikey = get_iexapi_keys()\n",
        "    iexurl = 'https://cloud.iexapis.com/stable/stock/{ticker}/quote?token={token}'.format(\n",
        "                                ticker=ticker, token=iexapikey)\n",
        "    return requests.get(iexurl).json()\n",
        "\n",
        "\n",
        "def get_all_prices():\n",
        "  all_stocks = requests.get('https://financialmodelingprep.com/api/v3/company/stock/list?apikey={}'.format(getfmpkeys())).json()['symbolsList']\n",
        "  tickers = list(map(lambda d: d['symbol'], all_stocks))\n",
        "  for t in tickers:\n",
        "    try:\n",
        "      get_prices(t)\n",
        "    except Exception as e:\n",
        "      print('unable to find {}:{}'.format(t, str(e)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2JKDUVNdlh",
        "colab_type": "text"
      },
      "source": [
        "<h3>IEX API CALLS </h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e18EfrivqJ_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "token = get_iexapi_keys()\n",
        "\n",
        "def get_statistics(ticker):\n",
        "  base_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/stats?token={token}&format=csv&filter=companyName,symbol,beta,day50MovingAvg,day200MovingAvg,month6ChangePercent,month3ChangePercent,month1ChangePercent'.format(token=token,symbol=ticker)\n",
        "  df = pd.read_csv(base_url)\n",
        "  df['Symbol'] = ticker\n",
        "  return df\n",
        "\n",
        "def get_iex_historical_data(ticker, nummonths):\n",
        "  return requests.get('https://cloud.iexapis.com/stable/stock/{ticker}/chart/{months}m?token={token}&chartCloseOnly=true'.format(token=token,\n",
        "                                                                                                                          ticker=ticker,\n",
        "                                                                                                                          months= nummonths)).json()\n",
        "  \n",
        "\n",
        "def get_all_stocks():\n",
        "  all_symbols_data = requests.get('https://cloud.iexapis.com/stable/ref-data/iex/symbols?token={token}'.format(token=token)).json()\n",
        "  return [d['symbol'] for d in all_symbols_data if d['isEnabled']and d['type'].lower() == 'cs']\n",
        "\n",
        "def get_all_us_stocks(security_type='cs', nasdaq=False):\n",
        "  nyse_symbols = requests.get('https://cloud.iexapis.com/stable/ref-data/exchange/nys/symbols?token={token}'.format(token=token)).json()\n",
        "  nas_symbols = requests.get('https://cloud.iexapis.com/stable/ref-data/exchange/nas/symbols?token={token}'.format(token=token)).json()\n",
        "  all_symbols = nyse_symbols + nas_symbols if nasdaq else nyse_symbols\n",
        "  return [d['symbol'] for d in all_symbols  if d['type'].lower() == security_type]\n",
        "\n",
        "def get_all_etfs():\n",
        "  return get_all_us_stocks(security_type='et', nasdaq=True)\n",
        "  \n",
        "def get_all_stocks_data():\n",
        "  good_ones = get_all_etfs()\n",
        "  return map(lambda symbol: (symbol, get_historical_value(symbol)), good_ones)\n",
        "\n",
        "\n",
        "def get_all_exchanges():\n",
        "  return requests.get('https://cloud.iexapis.com/stable/ref-data/market/us/exchanges?token={token}'.format(token=token)).json()\n",
        "\n",
        "def get_latest_price(symbol):\n",
        "  base_url = \"https://cloud.iexapis.com/stable/stock/{ticker}/quote?token={token}&format=csv&filter=symbol,close\".format(token=token,ticker=symbol)\n",
        "  import requests\n",
        "  return pd.read_csv(base_url)\n",
        "\n",
        "def get_quote(symbol):\n",
        "  try:\n",
        "    historical_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/quote/latestPrice?token={token}'.format(token=token,symbol=symbol)\n",
        "    return requests.get(historical_url).json()\n",
        "  except:\n",
        "    return -1\n",
        "\n",
        "def get_news(symbol, num_of_news):\n",
        "  try:\n",
        "    news_url = 'https://cloud.iexapis.com/stable//stock/{symbol}/news/last/{last}?token={token}'.format(symbol=symbol, last=num_of_news,\n",
        "                                                                                                        token=token)\n",
        "    return requests.get(news_url).json()\n",
        "  except Exception as e :\n",
        "    print('Excepiton for {}:{}'.format(symbol, str(e)))\n",
        "    return []\n",
        "\n",
        "def get_prices(ticker, iexkey):\n",
        "  try:\n",
        "    stat_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/quote?token={token}'.format(symbol=ticker, token=iexkey)\n",
        "    return requests.get(stat_url).json()\n",
        "  except Exception as e :\n",
        "    print('Excepiton for {}:{}'.format(symbol, str(e)))\n",
        "    return []\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzqkOp1BsG7g",
        "colab_type": "text"
      },
      "source": [
        "<h3> Yahoo API Calls </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bNDGbCNbX6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "import requests\n",
        "\n",
        "def get_latest_price_yahoo(symbol, as_of_date):\n",
        "  try:#\n",
        "    print('--latest price for{}'.format(symbol))\n",
        "    res = dr.get_data_yahoo(symbol, as_of_date, as_of_date)[['Close']]\n",
        "    df['Symbol'] = symbol\n",
        "    return df\n",
        "  except Exception as e :\n",
        "    return pd.DataFrame(columns=[symbol])\n",
        "\n",
        "def get_historical_data_yahoo(symbol, start_dt, end_dt):\n",
        "  try: \n",
        "    end_date = date.today()\n",
        "    data = dr.get_data_yahoo(symbol, start_dt, end_dt)[['Adj Close']]\n",
        "    df =  data.rename(columns={'Adj Close' : symbol})\n",
        "    return df\n",
        "  except Exception as e :\n",
        "    return pd.DataFrame(columns=[symbol])   \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqeae1y26bg-",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting Sentiment Analysis from Vadder </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeEBAuPm6f7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "# This is good one, Free\n",
        "\n",
        "def get_sentiment_from_vader(sentence):\n",
        "    analyser = SentimentIntensityAnalyzer()\n",
        "    score = analyser.polarity_scores(sentence)\n",
        "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
        "    return score['compound']\n",
        "\n",
        "get_sentiment_from_vader(\"Here's what booming dollar store sales say about America right now Yahoo Finance\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lVNnbmlOiUe",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting Sentiment Analysis from Google </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCwPBOkWsapp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentiment_from_google(content):\n",
        "  from google.cloud import language\n",
        "  from google.cloud.language import enums\n",
        "  from google.cloud.language import types\n",
        "\n",
        "  client = language.LanguageServiceClient()\n",
        "  document = types.Document(\n",
        "      content=clean_text,\n",
        "      type=enums.Document.Type.PLAIN_TEXT)\n",
        "\n",
        "  # Detects the sentiment of the text\n",
        "  sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjTo6XOKimci",
        "colab_type": "text"
      },
      "source": [
        "<h3> Google Language API </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-_MtxkZirQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_language_api(content):\n",
        "  import httplib2\n",
        "  import sys\n",
        "  from googleapiclient import discovery\n",
        "  from googleapiclient.errors import HttpError\n",
        "\n",
        "  discovery_url = 'https://{api}.googleapis.com/$discovery/rest?version={apiVersion}'\n",
        "\n",
        "  service = discovery.build(\n",
        "      'language', 'v1',\n",
        "      http=httplib2.Http(),\n",
        "      discoveryServiceUrl=discovery_url,\n",
        "      developerKey=get_nlp_service_keys(),\n",
        "  )\n",
        "  service_request = service.documents().annotateText(\n",
        "      body={\n",
        "          'document': {\n",
        "              'type': 'PLAIN_TEXT',\n",
        "              'content': content,\n",
        "          },\n",
        "          'features': {\n",
        "              'extract_syntax': True,\n",
        "              'extractEntities': True,\n",
        "              'extractDocumentSentiment': True,\n",
        "          },\n",
        "          'encodingType': 'UTF16' if sys.maxunicode == 65535 else 'UTF32',\n",
        "      })\n",
        "  try:\n",
        "      #print('************************')\n",
        "      #print('Retrieving sentiment for:{}'.format(content))\n",
        "      response = service_request.execute()\n",
        "      return  response['documentSentiment']['score']\n",
        "      \n",
        "  except HttpError as e:\n",
        "      response = {'error': e}\n",
        "      print('exception:{}'.format(str(e)))\n",
        "      return 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNkdWNJri04r",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting News from News API <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg8OLEEKi5qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_from_news_table(news_tables):\n",
        "  parsed_news = []\n",
        "  print('Found:{}'.format(len(news_tables.items())))\n",
        "  for file_name, news_table in news_tables.items():\n",
        "      for x in news_table.findAll('tr'):\n",
        "      \n",
        "          text = x.a.get_text() \n",
        "          date_scrape = x.td.text.split()\n",
        "\n",
        "          if len(date_scrape) == 1:\n",
        "              time = date_scrape[0]\n",
        "              \n",
        "          else:\n",
        "              date = date_scrape[0]\n",
        "              time = date_scrape[1]\n",
        "          ticker = file_name.split('_')[0]\n",
        "          \n",
        "          parsed_news.append([ticker, date, time, text])\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def retrieve_all_news(symbol):\n",
        "  print('Retrieving all news for@{}'.format(symbol))\n",
        "  token = get_newsapi_keys()\n",
        "  all_news = 'https://newsapi.org/v2/everything?q={ticker}&apiKey={token}'.format(ticker=symbol, token=token)\n",
        "  data = requests.get(all_news).json()\n",
        "  res = data['articles']\n",
        "  return map(lambda data: data['content'], res)\n",
        "\n",
        "def get_news_from_finviz(tickers):\n",
        "  news_tables = {}\n",
        "  for ticker in tickers:\n",
        "    url = 'https://finviz.com/quote.ashx?t={}'.format(ticker)\n",
        "    try:\n",
        "      req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
        "      response = urlopen(req)    \n",
        "      html = BeautifulSoup(response)\n",
        "      print('Now parsing...')\n",
        "      news_table = html.find(id='news-table')\n",
        "      news_tables[ticker] = news_table\n",
        "    except Exception as e:\n",
        "      print('culd not fetch data for {}:{}'.format(ticker, str(e)))\n",
        "    \n",
        "  return extract_from_news_table(news_tables)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_sentiment(news_items):\n",
        "  res = map(lambda item: get_sentiment_from_vader(item), news_items)\n",
        "  all_news =  list(res)\n",
        "  pprint('Total:{}'.format(all_news))\n",
        "  return dict(total=sum(all_news), positive=[i for i in all_news if i > 0], negative=[i for i in all_news if i <0])\n",
        "\n",
        "def calculate_news_sentiment(ticker):\n",
        "  latest_news = list(retrieve_all_news(ticker))\n",
        "  return calculate_sentiment(latest_news)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATbImVkuBJp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://datascience-bucket-mm/all_sectors.csv .\n",
        "sct = pd.read_csv('all_sectors.csv')\n",
        "sct.rename({'Unnamed: 0' : 'ticker', 'Sector':'sector'}, axis=1, inplace=True)\n",
        "\n",
        "cons_dens_ticker = sct[sct['sector'] == 'Consumer Defensive']\n",
        "\n",
        "cons_dens_ticker.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryRZyEHg_gLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_news_scores_for_ticker(tickers):\n",
        "  parsed_news = get_news_from_finviz(tickers)\n",
        "  columns = ['ticker', 'date', 'time', 'headline']\n",
        "  parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
        "  parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
        "  \n",
        "  return parsed_and_scored\n",
        "\n",
        "  parsed_and_scored_news_grouped = parsed_and_scored_news.groupby(['ticker'], as_index = False).agg({'headline': ''.join}, Inplace=True)\n",
        "  scores = parsed_and_scored_news_grouped['headline'].apply(get_sentiment_from_vader).tolist()\n",
        "  scores_df = pd.DataFrame(scores)\n",
        "  parsed_and_scored_news_result = parsed_and_scored_news_grouped.join(scores_df, rsuffix='_right')\n",
        "  return parsed_and_scored_news, parsed_and_scored_news_result\n",
        "\n",
        "\n",
        "tickers = cons_dens_ticker['ticker'].values.tolist()\n",
        "\n",
        "\n",
        "raw = find_news_scores_for_ticker(tickers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfPoshB9DUlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Below does notwork. finviz for ticker has different structure from finviz for general data.\n",
        "base_url = 'https://www.finviz.com/news.ashx'\n",
        "req = Request(url=base_url,headers={'user-agent': 'my-app/0.0.1'}) \n",
        "response = urlopen(req)    \n",
        "html = BeautifulSoup(response)\n",
        "print('Now parsing...')\n",
        "news_table = html.find(id='news-table')\n",
        "news_table\n",
        "#extract_from_news_table(news_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evmkXOrG1fBO",
        "colab_type": "text"
      },
      "source": [
        "<h3> Fetching Data From BigQuery </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyBaeqZv1dNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import seaborn as sns\n",
        "from matplotlib.pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "client = bigquery.Client('datascience-projects')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHwJrfQa2aQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aggregate(df):\n",
        "  print('Aggregating by ticker')\n",
        "  renamed = df.rename({'TICKER': 'Ticker'}, axis=1)\n",
        "  return renamed.groupby('Ticker', as_index=False)['COUNT'].sum()\\\n",
        "                              .rename(columns={'sum':'TOTAL_FILINGS'})\n",
        "  \n",
        "  #return renamed.groupby('Ticker', as_index=False).COUNT.agg(['sum']).rename({'sum' : 'TOTAL_FILINGS'},axis=1)\n",
        "\n",
        "def get_edgar_data():\n",
        "  sql = \"\"\"\n",
        "    SELECT *\n",
        "    FROM\n",
        "        `datascience-projects.gcp_edgar.form_13hf_daily`\n",
        "      \n",
        "\n",
        "    \"\"\"\n",
        "  df = client.query(sql).to_dataframe()\n",
        "  df['asofdate'] = pd.to_datetime(df['COB'], format='%Y-%m-%d')\n",
        "\n",
        "  print('Only considering last 2 months worth of iling')\n",
        "  start_date = date.today() - BDay(40)\n",
        "\n",
        "  filtered = df[df['asofdate']> start_date]\n",
        "  \n",
        "  return aggregate(filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AcdSMLd6mlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "relevant = get_edgar_data()\n",
        "relevant[relevant['Ticker'].isin(['NIO', 'MGM'])].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1KZ5kLV98U",
        "colab_type": "text"
      },
      "source": [
        "<h3>Computing various  metrics </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvySZQvWBiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need to sort out which function we need.\n",
        "#1. calculate sharpe ratio  DONE\n",
        "#2. get month change percent m30\n",
        "#3. get full performance   DONE\n",
        "#4. get news  DONE\n",
        "#5 group together and compute\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "from math import sqrt\n",
        "from pprint import pprint\n",
        "\n",
        "def calculate_daily_returns(prices):\n",
        "  return prices.pct_change(1)\n",
        "\n",
        "def calculate_daily_cumulative_returns(daily_pc):\n",
        "  return (1 + daily_pc).cumprod()\n",
        "\n",
        "def compute_standard_deviation(daily):\n",
        "  return daily.loc[:,daily.columns[0]].std()\n",
        "\n",
        "def compute_sharpe_ratio(s_prices):\n",
        "  # This function should be used in final dataframe\n",
        "  # USE THSI FUNCTION\n",
        "  dret = calculate_daily_returns(s_prices)\n",
        "  avg = dret.loc[:,dret.columns[0]].mean()\n",
        "  std = compute_standard_deviation(dret)\n",
        "  return (sqrt(252) * avg) / std\n",
        "\n",
        "def compute_moving_averages(prices, day):\n",
        "  return prices.rolling(window=day).mean()\n",
        "\n",
        "def check_prices_vs_moving_averages(prices, day=30):\n",
        "  # This Function should be used  in final dataframe\n",
        "  ma30 = compute_moving_averages(prices, 30)\n",
        "  ticker_col = ma30.columns[0]\n",
        "  m30_col = '{}M30'.format(ticker_col)\n",
        "  ma30_renamed = ma30.rename({ticker_col: m30_col}, axis=1)\n",
        "  concats = pd.concat([prices, ma30_renamed], axis=1)\n",
        "  concats['AboveM30'] = concats[ticker_col] > concats[m30_col]\n",
        "  above_m30 = concats[concats['AboveM30'] == True]\n",
        "  total_prices = prices.shape[0]\n",
        "  total_m30 = above_m30.shape[0]\n",
        "  pcnt = 1.0*total_m30/total_prices\n",
        "  return pcnt\n",
        "\n",
        "def compute_data_performance(historical_df, ticker):\n",
        "  # Use this FUNCTION CALL\n",
        "  start = historical_df[ticker].values[0]\n",
        "  end = historical_df[ticker].values[-1]\n",
        "  return end*1.0/start - 1\n",
        "\n",
        "def compute_metrics(prices):\n",
        "  # Add Trade Volumne to spot momentum stocks\n",
        "  ticker = prices.columns[0]\n",
        "  perf_dict = {}\n",
        "  perf_dict['Ticker'] = ticker\n",
        "  perf_dict['Performance'] = compute_data_performance(prices, ticker)\n",
        "  perf_dict['Start_Price'] = prices[ticker].values[0]\n",
        "  perf_dict['End_Price'] = prices[ticker].values[-1]\n",
        "  #perf_dict['AboveMovingAvgPcnt'] = check_prices_vs_moving_averages(prices, day=15)\n",
        "  perf_dict['SharpeRatio'] = compute_sharpe_ratio(prices)\n",
        "  #news_dict =calculate_news_sentiment(ticker)\n",
        "  #news_measure = sum(news_dict['positive']) + sum(news_dict['negative'])\n",
        "  #perf_dict['News_Sentiment'] = news_measure\n",
        "  return pd.DataFrame([perf_dict])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTZu-ZatcHOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# get yahoo historical data\n",
        "def get_data(ticker, dt, busdays=1):\n",
        "  res = dr.get_data_yahoo(ticker, dt, dt)[['Adj Close', 'Volume']]\n",
        "  res['Symbol'] = ticker\n",
        "  return res\n",
        "\n",
        "def get_share_data(ticker):\n",
        "  from datetime import date\n",
        "  today = date.today()\n",
        "  start_date = today- BDay(1)\n",
        "  today_df = get_data(ticker, today)   \n",
        "  print(today_df.head(2))\n",
        "  yday_df = get_data(ticker, start_date)\n",
        "  yday_df = yday_df.rename(columns={\"Adj Close\": \"Prev Close\", \"Volume\": \"Prev Volume\"})\n",
        "  print('Merging')\n",
        "  merged = pd.merge(today_df, yday_df, on='Symbol')\n",
        "  print('Calclating diff')\n",
        "  merged['Diff'] = merged['Adj Close'] - merged['Prev Close']\n",
        "  merged['Vol Diff'] = merged['Volume'] - merged['Prev Volume']\n",
        "  return merged\n",
        "#compute_metrics(historical_df)\n",
        "#pd.DataFrame.from_dict(res)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSJQ73Am2dC",
        "colab_type": "text"
      },
      "source": [
        "<h3> Reading source data and computing performance </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euF2kHI-n4qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import date\n",
        "def get_date_ranges(prev_bus_days):\n",
        "  print('Checking result sfor {}'.format(prev_bus_days))\n",
        "  end_date = date.today()\n",
        "  start_date = end_date - BDay(prev_bus_days) # go back 3 months to find crossovers. Start to plan to move to Dataflow\n",
        "  return start_date, end_date\n",
        "\n",
        "def compute_performance(idx, start_dt, end_dt, ticker):\n",
        "  try:\n",
        "    print('Computing {}'.format(idx))\n",
        "    # Here we need to compute, in addition to performance, also sharpe ratio etc.\n",
        "    import time\n",
        "    historical_df =  get_historical_data_yahoo(ticker, start_dt, end_dt)\n",
        "    #merged = pd.merge(historical_df, latest_df, how='inner' , on=ticker)\n",
        "    metrics_df = compute_metrics(historical_df)\n",
        "    return metrics_df\n",
        "  except Exception as e:\n",
        "    print('Exception:{}'.format(str(e)))\n",
        "    print('Unable to find data for {}:{}'.format(ticker,str(e)))\n",
        "    \n",
        "def find_best_performing(start_dt, end_dt, symbols):\n",
        "  print('Finding Best Performing Stocks between:{}-{}'.format(start_dt, end_dt))\n",
        "  # Attempting 100 at a time\n",
        "  # Targeting run on 15/16/17/18 in lot fo 400 at a time, forcing start date to be the friday before so\n",
        "  # we can cover all news as we have a max request of 500\n",
        "  \n",
        "  dfs = (compute_performance(idx, start_dt, end_dt, symbol) for idx, symbol in enumerate(symbols))\n",
        "  filtered = (df for df in dfs if df is not None)\n",
        "  all_data = pd.concat(filtered)\n",
        "  #all_data = all_data[(all_data.Start_Price >= 10)]\n",
        "  all_data.sort_values(by=['Performance'], inplace=True, ascending=False)\n",
        "  return all_data\n",
        "  \n",
        "#performance_df.to_csv('performance_run_20200207-news-first300.csv')\n",
        "#!gsutil cp performance_run_20200207-news-first300.csv gs://datascience-bucket-mm/monthly_run/performance_run_20200207-news-first300.csv\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rLcKk52kBqU",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting all sectors and indicators datset </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7fPfbPRkAfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data\n",
        "!gsutil cp gs://datascience-bucket-mm/all_sectors.csv data\n",
        "sct = pd.read_csv('data/all_sectors.csv')\n",
        "sct.rename({'Unnamed: 0' : 'ticker', 'Sector':'sector'}, axis=1, inplace=True)\n",
        "print(sct.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFRO3PqvkMwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_performance_for_sector(sector_name, all_sector_df):\n",
        "  print('Checking perf of  {}'.format(sector_name))\n",
        "  all_data= []\n",
        "  s_df = all_sector_df[all_sector_df['sector'] == sector_name]\n",
        "  all_tickers = s_df.ticker.values.tolist()\n",
        "  start,end = get_date_ranges(20)\n",
        "  for idx, ticker in enumerate(all_tickers):\n",
        "    print(ticker)\n",
        "    res = compute_performance(idx, start, end, ticker)\n",
        "    if res is not None:\n",
        "      all_data.append(res)\n",
        "  \n",
        "  if all_data:\n",
        "    res =  pd.concat([p for p in all_data], axis=0)\n",
        "    res['sector'] = sector_name\n",
        "    return res\n",
        "  else:\n",
        "    print('===== NO DATA FOUND FOR SECTOR {}={}'.format(sector_name, len(all_tickers)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K2Kzyn_mKDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_sectors = set([s for s in sct.sector.dropna().values.tolist() if s != 'nan'])\n",
        "first_5to8_dfs = [compute_performance_for_sector(s, sct) for s in list(all_sectors)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P17HSZw825I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_5_res = pd.concat([df for df in first_5to8_dfs if df is not None])\n",
        "first_5_res.to_csv('august__end_run_all_perf.csv')\n",
        "first_5_res.shape\n",
        "!gsutil cp august__end_run_all_perf.csv gs://datascience-bucket-mm/monthly_run/\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVzcV_l5RiSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp  gs://datascience-bucket-mm/monthly_run/august__end_run_all_perf.csv .\n",
        "\n",
        "df = pd.read_csv('august__end_run_all_perf.csv')\n",
        "#df[df['Ticker'].isin(['KR','WST','AMGN','FAST','DXCM','DOCU','GMAB'])]\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4o1A6xv3UMH",
        "colab_type": "text"
      },
      "source": [
        "<h3> Get Edgar Data </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVaf2eQm3XMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "edgar_filing = get_edgar_data().rename({'TICKER':'Ticker'}, axis=1)\n",
        "edgar_filing.head(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp3nyjhKkL-g",
        "colab_type": "text"
      },
      "source": [
        "<h3> Sector Performance </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74xN7yvqkINB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupby(['sector']).mean().sort_values(by=['Performance'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU1zfdbtiR5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug_shares = df[(df['sector'] == 'Consumer Cyclical') & (df.Start_Price > 10)]\n",
        "\n",
        "\n",
        "merged = aug_shares.merge(edgar_filing, on='Ticker', how='left').fillna(0)\\\n",
        "           .sort_values(by=['Performance', 'COUNT'], ascending=False).head(30)\n",
        "\n",
        "merged\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR-gFNQElNJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "edgar_filing.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se7NL4BNSnHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start,end = get_date_ranges(20)\n",
        "compute_performance(0, start, end, 'KR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDZp3F-Z126V",
        "colab_type": "text"
      },
      "source": [
        "<h3> Copying exported files here </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4pupbLOkrLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_sectors = sct['sector'].dropna().unique().tolist()\n",
        "print(all_sectors)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3sB-RQ3B3GU",
        "colab_type": "text"
      },
      "source": [
        "<h3> Finding best performers among shares </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MAsss2-B2Jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_dt, end_dt = get_date_ranges(60)\n",
        "stocks = get_all_us_stocks()\n",
        "print('We got:{}'.format(len(stocks)))\n",
        "performance_df = find_best_performing(start_dt, end_dt, stocks[0:10])\n",
        "\n",
        "#print(performance_df.shape)\n",
        "performance_df.head(10)\n",
        "performance_df.to_csv('performance_run_20200306_60_days.csv')\n",
        "!gsutil cp performance_run_20200302.csv gs://datascience-bucket-mm/monthly_run/performance_run_20200302.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_V3ZFA_ldRY",
        "colab_type": "text"
      },
      "source": [
        "<h3> Crssing over with stocks which are still growing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QcpBoD-jlb7n",
        "colab": {}
      },
      "source": [
        "start_dt, end_dt = get_date_ranges(30)\n",
        "stocks = get_all_us_stocks()\n",
        "all_etfs = get_all_etfs()\n",
        "all_stocks = stocks + all_etfs\n",
        "print('We got:{}'.format(len(all_stocks)))\n",
        "performance_df_recent = find_best_performing(start_dt, end_dt, all_stocks)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmT5Iq6fXPBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(performance_df.shape)\n",
        "sorted_performance = performance_df_recent.sort_values(by=['Performance'], ascending=False)\n",
        "print(sorted_performance.head(10))\n",
        "sorted_performance.to_csv('performance_run_20200318_30_days.csv')\n",
        "!gsutil cp performance_run_20200318_30_days.csv gs://datascience-bucket-mm/monthly_run/performance_run_20200318_30_days.csvs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC32mz-7qP1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sorted_performance.head(30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTp_nzxVo-eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "find_best_performing(start_dt, end_dt,  ['TVIX'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjc6cy3-uAMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "good_60 = performance_df[performance_df['Performance'] > 0]\n",
        "good_20= performance_df_recent[performance_df_recent['Performance'] > 0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXkQqpiOuk3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged = good_20.merge(good_60, on='Ticker', how='inner', suffixes=['_20','_60'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyFlzQR_vaAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged.to_csv('performance_run_20200306_60_vs_20days.csv')\n",
        "!gsutil cp performance_run_20200306_60_vs_20days.csv gs://datascience-bucket-mm/monthly_run/performance_run_20200306_60_vs_20days.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOKHbr85CISm",
        "colab_type": "text"
      },
      "source": [
        "<h3> Finding best performenrs among ETF </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU0aptYMwg4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_etfs = get_all_etfs()\n",
        "start_dt, end_dt = get_date_ranges(30)\n",
        "performance_df_etf_60 = find_best_performing(start_dt, end_dt, all_etfs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1RG5to2gZxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "setf_performance = performance_df_etf_60.sort_values(by=['Performance'], ascending=False)\n",
        "print(setf_performance.head(20))\n",
        "setf_performance.to_csv('performance_etf_run_20200318_30_days.csv')\n",
        "!gsutil cp performance_etf_run_20200318_30_days.csv gs://datascience-bucket-mm/monthly_run/performance_etf_run_20200318_30_days.csvs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24QenawhvwVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_dt, end_dt = get_date_ranges(20)\n",
        "performance_df_etf_20 = find_best_performing(start_dt, end_dt, all_etfs)\n",
        "good_etf60 = performance_df_etf_60[performance_df_etf_60['Performance'] > 0]\n",
        "good_etf20= performance_df_etf_20[performance_df_etf_20['Performance'] > 0]\n",
        "merged_etf = good_etf20.merge(good_etf60, on='Ticker', how='inner', suffixes=['_20','_60'])\n",
        "merged_etf.to_csv('performance_run_20200306_60_vs_20days_etf.csv')\n",
        "!gsutil cp performance_run_20200306_60_vs_20days_etf.csv gs://datascience-bucket-mm/monthly_run/performance_run_20200306_60_vs_20days_etf.csv\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GGRcMTbwhCj",
        "colab_type": "text"
      },
      "source": [
        "<h2> TODO: Fetch news for every ticker and find out sentiment. then build a dataframe of symbol, positive news, performance </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEBMn4Zlkjgf",
        "colab_type": "text"
      },
      "source": [
        "<h3> Group by sector, to find best performers </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BrXBHSsqa9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = perf_df[['industry', 'month1ChangePercent','month3ChangePercent', ]].groupby(['industry']).mean().sort_values(by=['month1ChangePercent','month1ChangePercent'], ascending=False)\n",
        "res.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNrTkBIcADGb",
        "colab_type": "text"
      },
      "source": [
        "<p> Testing all stocks in portfolio </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oildRTtsAIh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "portfolio_shares = ['ADAC', 'AMBS', 'AMZN', 'AZFL', 'ARSC', 'AAPL', 'APTY',\n",
        "                    'BTCS', 'BRK-B', 'CRNT', 'CRLBF', 'XOM', 'HAON', 'AGEEF',\n",
        "                    'HMNY', 'JNJ', 'LEMIF', 'NXTTF', 'NVCN', 'RNVA', 'TORC',\n",
        "                    'RTRX', 'VALE', 'VZ', 'DGP', 'RUSL', 'REMX', 'TVIX' ]\n",
        "\n",
        "all_shares = get_all_stocks()\n",
        "res = map(lambda symbol:(symbol, symbol in all_shares), shares)\n",
        "invalid = [tpl for tpl in res if not tpl[1]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT-4mE-grwDa",
        "colab_type": "text"
      },
      "source": [
        "<h3> Performance Functions </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksISPaesqPEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_correlation_all(vix, all_stocks):\n",
        "  all_df = [vix]\n",
        "  res= [vals for _, vals in all_stocks if vals.shape[0] == vix_vals.shape[0]]\n",
        "  res.append(vix)\n",
        "  all_data = pd.concat(res, axis=1)\n",
        "  return all_data.corr('pearson')\n",
        "\n",
        "def calculate_portfolio_correlation(all_stocks):\n",
        "  res= [vals for _, vals in all_stocks if vals.shape[0] > 2]\n",
        "  all_data = pd.concat(res, axis=1)\n",
        "  return all_data.corr('pearson')\n",
        "\n",
        "\n",
        "def calculate_correlation(vix, all_stocks):\n",
        "  result = []\n",
        "  best = 0\n",
        "  for symbol, vals in all_stocks:\n",
        "    if vals.shape[0] == vix.shape[0]:\n",
        "      concats  = pd.concat([vix, vals], axis = 1)\n",
        "      corr_matrix = concats.corr(method='pearson')\n",
        "      corr_with_vix = corr_matrix.loc['^VIX'][1]\n",
        "      if corr_with_vix > 0 and corr_with_vix > best:\n",
        "        print('New Corr with {}:{}'.format(symbol, corr_with_vix))\n",
        "        best = corr_with_vix\n",
        "  return best\n",
        "\n",
        "def _get_most_correlated(result_df):\n",
        "  df = result_df[['^VIX']]\n",
        "  bad_df = df.index.isin(['^VIX'])\n",
        "  return df[~bad_df]\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpVVvwqxGYc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vix_vals = get_historical_value('^VIX')\n",
        "\n",
        "all_stocks_data = map(lambda symbol: (symbol, get_historical_value(symbol)), portfolio_shares)\n",
        "best = calculate_portfolio_correlation(all_stocks_data)      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSMzmPKiY-Yb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNX-4-970hiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = _get_most_correlated(best)\n",
        "sorted_df = res.sort_values('^VIX', ascending=False)\n",
        "sorted_df.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ-fsQbw3o53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = sorted_df.iloc[0]\n",
        "\n",
        "print(idx.values.tolist()[0])\n",
        "print(sorted_df.index[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o3uRW6bjOTA",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting All US Stocks </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COGKQLeXjThy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iexapikey = get_iexapi_keys()\n",
        "iexurl = 'https://cloud.iexapis.com/stable/stock/market/sector-performance?token={token}'.format(\n",
        "                              token=iexapikey)\n",
        "requests.get(iexurl).json()\n",
        "#TODO we need to find etf tracking different sectors..\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mN55XgONj1J",
        "colab_type": "text"
      },
      "source": [
        "<h3> REtrieving News </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWXvmySV-ehR",
        "colab_type": "text"
      },
      "source": [
        "<h3> Accessing bigQuery </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnj2LG6k-ghv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rmsRS_1-sPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project datascience-projects --use_legacy_sql --verbose df\n",
        "SELECT \n",
        "  COB, COUNT(*) as total_rows\n",
        "FROM [datascience-projects.gcp_edgar.form_13hf_data]  GROUP BY COB;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxdctIbCA7bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jQNh5FmBH9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0JE0ycbnFc5",
        "colab_type": "text"
      },
      "source": [
        "<h3> GEtting quartely reports from IEXAPI </h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gENI3UgBnKIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_last_quartely_statements(ticker, num_statements):\n",
        "  print('Fetching last {} statements for {}'.format(num_statements, ticker))\n",
        "  q_url = 'https://cloud.iexapis.com/stable/time-series/REPORTED_FINANCIALS/{}/10-Q?last={}&token={}'.format(ticker, num_statements, get_iexapi_keys())\n",
        "  print('Loading up {}'.format(q_url))\n",
        "  return requests.get(q_url).json()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nGwqdHJoP4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'ticker' in df.columns.values\n",
        "df.to_csv('aapl_last_31_10Q_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndMXyqRRqPR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp 'aapl_last_31_10Q_dataset.csv'  gs://mm_dataflow_bucket/inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIkvH_ZkrC6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://datascience-bucket-mm/all_sectors.csv .\n",
        "sct = pd.read_csv('all_sectors.csv')\n",
        "sct.rename({'Unnamed: 0' : 'ticker', 'Sector':'sector'}, axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxlCF39ptEjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "grouped = sct.groupby('sector').aggregate(lambda tdf: tdf.unique().tolist()[0:5])\n",
        "\n",
        "sample_tickers = grouped['ticker'].values\n",
        "all_ticks = list(chain(*sample_tickers))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCAGlSWL65bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grouped.to_csv('random_tickers_for_sector.csv')\n",
        "!gsutil cp 'random_tickers_for_sector.csv'  gs://mm_dataflow_bucket/inputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZN8tJjU6qjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "ticker_dict = {}\n",
        "for ticker in all_ticks:\n",
        "  latest = get_last_quartely_statements(ticker, 32)\n",
        "  df = pd.DataFrame(latest)\n",
        "  print('REsult for {} has shape:{}'.format(ticker, df.shape))\n",
        "  ticker_dict[ticker] = df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lItuPZf38pBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = ticker_dict.items()\n",
        "len(items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9W6PMF09uJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "common_cols_dict = dict()\n",
        "for k, v in ticker_dict.items():\n",
        "  if v.shape[0] > 1:\n",
        "    print('Adding {} with shape {}'.format(k, v.shape))\n",
        "    common_cols_dict[k] = v\n",
        "  else:\n",
        "    print('Skipping {}, its empty'.format(ticker))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnBOlXDe-Qrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_items_sorted= sorted(common_cols_dict.items(), key=lambda t: t[1].shape[1], reverse=False)\n",
        "\n",
        "common_cols = all_items_sorted[0][1].columns.values\n",
        "for k, df in all_items_sorted[1:]:\n",
        "  current_cols = df.columns.values\n",
        "  print('({}) - common:{}. current:{}'.format(k, len(common_cols), len(current_cols)))\n",
        "  itsect = set(common_cols).intersection(set(current_cols))\n",
        "  r_itsect = set(current_cols).intersection(set(common_cols))\n",
        "  \n",
        "  smallest = itsect if itsect < r_itsect else r_itsect\n",
        "  common_cols = smallest\n",
        "  print('Now common_cols has:{}'.format(len(common_cols)))\n",
        "\n",
        "print('Now making sure everyone is ok')\n",
        "unioned = []\n",
        "for k, df in all_items_sorted:\n",
        "  try:\n",
        "    selected = df[common_cols]\n",
        "    selected['ticker'] = k\n",
        "    unioned.append(selected) \n",
        "  except Exception as e:\n",
        "    print('{} exception {}'.format(k, str(e)))\n",
        "\n",
        "final_df = pd.concat(unioned)\n",
        "\n",
        "print('Final df  has:{}'.format(final_df.shape))\n",
        "\n",
        "final_df.to_csv('quarterly_earnings_mixed_tickets_dataset.csv')\n",
        "!gsutil cp 'quarterly_earnings_mixed_tickets_dataset.csv'  gs://mm_dataflow_bucket/inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "  \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v71nDb1CRHe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Now testing if those commoon cols are good enough fo all our datasets :{}'.format(len(common_cols)))\n",
        "base_cols = common_cols\n",
        "for ticker, df in ticker_dict.items():\n",
        "  if df.shape[0] > 0:\n",
        "    df_cols = df.columns.values\n",
        "    itsct = set(common_cols).intersection(set(df_cols))\n",
        "    if len(itsct) < len(base_cols):\n",
        "      print('UPdating base_cols to {}'.format(len(itsct)))\n",
        "      base_cols = itsct\n",
        "    \n",
        "print('And finally we have {}.Retrying'.format(len(base_cols)))\n",
        "new_dict = {}\n",
        "for ticker, df in ticker_dict.items():\n",
        "  if df.shape[0] > 0:\n",
        "    print('Testing {} for  common cols'.format(ticker))\n",
        "    try:\n",
        "      new_dict[ticker]  = df[base_cols]\n",
        "    except Exception as e:\n",
        "      print('{} Exception:{}'.format(ticker, str(e)))\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0yRihp9AR5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}