{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsSentimentAnalysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMB8eYf1I/akzLTnL/oRwBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/jupyter/blob/master/NewsSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHv6loOSfSr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas OpenBlender scikit-learn\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAXvSDgQl7ED",
        "colab_type": "text"
      },
      "source": [
        "<p> Not really applicable. We need to interact back and forth with OpenBlender. Trying Alternate API </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8brN2XfmFfI",
        "colab_type": "text"
      },
      "source": [
        "<h3> NewsAPI good but we have a maximum limit</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFFu6CZRmDnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def get_iexapi_keys():\n",
        "  with open('gdrive/My Drive/passwords/iexapi.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "def get_nlp_service_keys():\n",
        "  with open('gdrive/My Drive/passwords/nlp.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "def get_newsapi_keys():\n",
        "  with open('gdrive/My Drive/passwords/newsapi.keys') as f:\n",
        "    return f.readlines()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axoydb9zmYm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "def retrieve_all_news(symbol):\n",
        "  print('Retrieving all news for@{}'.format(symbol))\n",
        "  token = get_newsapi_keys()\n",
        "  all_news = 'https://newsapi.org/v2/everything?q={ticker}&apiKey={token}'.format(ticker=symbol, token=get_newsapi_keys())\n",
        "  data = requests.get(all_news).json()\n",
        "  res = data['articles']\n",
        "  return map(lambda data: data['content'], res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYS767wss0JK",
        "colab_type": "text"
      },
      "source": [
        "<h3> Fetching news from finviz (good but only applies to daily news )</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmh140RwnThe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from datetime import datetime, date\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen, Request\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import BDay\n",
        "# This is good one, Free\n",
        "\n",
        "def get_news_from_finviz2(tickers):\n",
        "  news_tables = {}\n",
        "  for ticker in tickers:\n",
        "    url = 'https://finviz.com/quote.ashx?t={}'.format(ticker)\n",
        "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
        "    response = urlopen(req)    \n",
        "    html = BeautifulSoup(response)\n",
        "    print('Now parsing...')\n",
        "    news_table = html.find(id='news-table')\n",
        "    news_tables[ticker] = news_table\n",
        "    \n",
        "  parsed_news = []\n",
        "  print('Found:{}'.format(len(news_tables.items())))\n",
        "  for file_name, news_table in news_tables.items():\n",
        "      for x in news_table.findAll('tr'):\n",
        "          text = x.a.get_text() \n",
        "          ticker = file_name.split('_')[0]\n",
        "          parsed_news.append([ticker, date, time, text])\n",
        "  return parsed_news\n",
        "\n",
        "\n",
        "\n",
        "def get_news_from_finviz(tickers):\n",
        "  news_tables = {}\n",
        "  for ticker in tickers:\n",
        "    url = 'https://finviz.com/quote.ashx?t={}'.format(ticker)\n",
        "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
        "    response = urlopen(req)    \n",
        "    html = BeautifulSoup(response)\n",
        "    print('Now parsing...')\n",
        "    news_table = html.find(id='news-table')\n",
        "    news_tables[ticker] = news_table\n",
        "    \n",
        "  parsed_news = []\n",
        "  print('Found:{}'.format(len(news_tables.items())))\n",
        "  for file_name, news_table in news_tables.items():\n",
        "      for x in news_table.findAll('tr'):\n",
        "      \n",
        "          text = x.a.get_text() \n",
        "          date_scrape = x.td.text.split()\n",
        "          link = x.a.get('href')\n",
        "          if len(date_scrape) == 1:\n",
        "              time = date_scrape[0]\n",
        "          else:\n",
        "              date = date_scrape[0]\n",
        "              time = date_scrape[1]\n",
        "          ticker = file_name.split('_')[0]\n",
        "          \n",
        "          parsed_news.append([ticker, date, time, text, link])\n",
        "  return parsed_news\n",
        "\n",
        "def get_sentiment_from_vader(sentence):\n",
        "    analyser = SentimentIntensityAnalyzer()\n",
        "    score = analyser.polarity_scores(sentence)\n",
        "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
        "    return score['compound']\n",
        "\n",
        "\n",
        "def find_news_scores_for_ticker(tickers):\n",
        "  try:\n",
        "    parsed_news = get_news_from_finviz(tickers)\n",
        "    #mapped_dt = list(map(lambda tpl:tpl[1], parsed_news))\n",
        "    #return mapped_dt\n",
        "  except Exception as e:\n",
        "    print('cant find naything for :{}'.format(tickers))\n",
        "    return []\n",
        "\n",
        "  columns = ['ticker', 'date', 'time', 'headline', 'link']\n",
        "  parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
        "  parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
        "  yday = date.today() - BDay(20)\n",
        "  filtered = parsed_and_scored_news[parsed_and_scored_news['date'] > yday]\n",
        "  #print(parsed_and_scored_news[['date', 'ticker', 'headline']].head())\n",
        "  parsed_and_scored_news = filtered.groupby(['ticker'], as_index = False).agg({'headline': ''.join}, Inplace=True)\n",
        "  scores = parsed_and_scored_news['headline'].apply(get_sentiment_from_vader).tolist()\n",
        "  scores_df = pd.DataFrame(scores)\n",
        "  parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
        "  return parsed_and_scored_news\n",
        "\n",
        "\n",
        "r = find_news_scores_for_ticker(['PTON'])\n",
        "print(type(r))\n",
        "r\n",
        "\n",
        "#get_sentiment_from_vader(\"Here's what booming dollar store sales say about America right now Yahoo Finance\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf38od01S6Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IqV52asM_2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting all sectors all companies\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!mkdir -p data\n",
        "!gsutil cp gs://datascience-bucket-mm/all_sectors.csv data\n",
        "sct = pd.read_csv('data/all_sectors.csv')\n",
        "sct.rename({'Unnamed: 0' : 'ticker', 'Sector':'sector'}, axis=1, inplace=True)\n",
        "print(sct.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRQVLJr2XiSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sct['sector'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bAbyRCBXS7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ccycles = sct[sct['sector'] == 'Utilities']\n",
        "ccycles.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwlDxUvHecnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = dict((tick,find_news_scores_for_ticker([tick]) ) for tick in ccycles.ticker.values.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y46_aCip8Efn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_df = [df for df in all_data.values() if isinstance(df,pd.DataFrame)]\n",
        "r = pd.concat(all_df, axis=0)\n",
        "r.shape\n",
        "r.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfs_NTWGCF_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tst = all_data['D']\n",
        "tst\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfaVng1cCRGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = tst.to_dict()\n",
        "dict((k, d[k][0]) for k in d.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNlk5I0RMS0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import _pickle as pickle\n",
        "with open(\"tick_to_news_august.pkl\", \"wb\") as fp:\n",
        "    pickle.dump(all_data, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHUtge7TTRxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r.to_csv('utilities_news.csv')\n",
        "!gsutil cp 'utilities_news.csv' gs://datascience-bucket-mm/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3SnYznwa3tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp  gs://datascience-bucket-mm/tick_to_news.pkl ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb1UILB72fSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import _pickle as pickle\n",
        "with open(\"tick_to_news.pkl\",'rb') as ts:\n",
        "  new_dict = pickle.load(ts)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz2F4uN2f1Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(new_dict.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XiS32SQc9Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_x = map(lambda tpl: (tpl[0],tpl[-1] if tpl[1] else None), new_dict.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keys6Jkse7dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first = list(all_x)[0:10]\n",
        "first"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH3HimXLfrjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(new_dict.keys())[0:2]\n",
        "for k in list(new_dict.keys())[0:10]:\n",
        "  print(new_dict[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtohT8GYNZkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_dict = [(k, ','.join(list(v)) if list(v) else None) for k, v in all_data.items()]\n",
        "df = pd.DataFrame(list_of_dict, columns=['ticker', 'oldest_news'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czytTMHqPG0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"all_ticker_news.csv\")\n",
        "!gsutil cp all_ticker_news.csv gs://datascience-bucket-mm/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPggI1bKu_-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://datascience-bucket-mm/all_ticker_news.csv ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNmBbOW3vWU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('all_ticker_news.csv')\n",
        "df.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zs3za9TzXy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['oldest_news']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9MNqYBdsAwg",
        "colab_type": "text"
      },
      "source": [
        "<h3>Historical news </h3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z55oLpQF4FVs",
        "colab_type": "text"
      },
      "source": [
        "<h4> Trying first iexcloud and see how much we use </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHVMHQQS3FoF",
        "colab_type": "text"
      },
      "source": [
        "<h3> Using BigQuery to estract dat aint oPandas </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krLRdtJoehhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbtjWl34eAH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import seaborn as sns\n",
        "from matplotlib.pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "client = bigquery.Client('datascience-projects')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O7DugGXe8iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sql = \"\"\"\n",
        "SELECT *\n",
        "FROM\n",
        "    `datascience-projects.gcp_edgar.form_13hf_daily`\n",
        "\n",
        "\"\"\"\n",
        "df = client.query(sql).to_dataframe()\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfTx0tEvfZQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['TICKER', 'CUSIP','COUNT']].groupby(['TICKER','CUSIP',  'COUNT']).sum().sort_values(by='COUNT', ascending=False).head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ezr_W2Jp-4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered = df[df['TICKER'] == 'MU'].sort_values(by='COB')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=filtered.COB, y=filtered.COUNT,\n",
        "                    mode='lines',\n",
        "                    name='close'))\n",
        "#fig.update_layout(showlegend=True)\n",
        "#fig.show()\n",
        "filtered\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gqMEEN03LYK",
        "colab_type": "text"
      },
      "source": [
        "<h3> Fetching analyst recommendations </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUCAbhrDhp4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "recommendations = []\n",
        "for ticker in ['AAPL', 'AMZN']:\n",
        "    lhs_url = 'https://query2.finance.yahoo.com/v10/finance/quoteSummary/'\n",
        "    rhs_url = '?formatted=true&crumb=swg7qs5y9UP&lang=en-US&region=US&' \\\n",
        "              'modules=upgradeDowngradeHistory,recommendationTrend,' \\\n",
        "              'financialData,earningsHistory,earningsTrend,industryTrend&' \\\n",
        "              'corsDomain=finance.yahoo.com'\n",
        "              \n",
        "    url =  lhs_url + ticker + rhs_url\n",
        "    r = requests.get(url)\n",
        "    if not r.ok:\n",
        "        recommendation = 6\n",
        "    try:\n",
        "        result = r.json()['quoteSummary']['result'][0]\n",
        "        recommendation =result['financialData']['recommendationMean']['fmt']\n",
        "    except:\n",
        "        recommendation = 6\n",
        "    \n",
        "    recommendations.append(recommendation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-stEeo9a3k-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recommendations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDUsTLf93vfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}