{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ApacheBeamNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/jupyter/blob/master/ApacheBeamNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYLkdcol0EU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbdMCgGYOYu-",
        "colab_type": "text"
      },
      "source": [
        "<h3> Installing apache beam </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN_VLiLKOiow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install apache-beam==2.19.0\n",
        "#!pip install google-cloud-bigquery==0.25.0\n",
        "#!pip install google-cloud-dataflow==2.4.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "174uLTmLop2T",
        "colab_type": "text"
      },
      "source": [
        "<h3> Setting google env variables </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXiNj7u5ot1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "PROJECT = \"datascience-projects\" # REPLACE WITH YOUR PROJECT ID\n",
        "BUCKET = \"mm_dataflow_bucket\" # REPLACE WITH YOUR BUCKET NAME\n",
        "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
        "\n",
        "# Do not change these\n",
        "os.environ[\"PROJECT\"] = PROJECT\n",
        "os.environ[\"BUCKET\"] = BUCKET\n",
        "os.environ[\"REGION\"] = REGION\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJnCvsO_pXM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c74-8qekOZF6",
        "colab_type": "text"
      },
      "source": [
        "<h3> Starting. Importing packages </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Mb92e2OVeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.metrics import Metrics\n",
        "from apache_beam.metrics.metric import MetricsFilter\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "from itertools import groupby"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAE_a4iwPm8W",
        "colab_type": "text"
      },
      "source": [
        "<h3> Creating directory to hold data </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqn0eRr7OJx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjGNq_s-Ptm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq4llbUMP1Kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing data into colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNR0IHL1Q5QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv dept-data.txt data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8JU7PwCGwWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls data\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IVJU7KhmWn9",
        "colab_type": "text"
      },
      "source": [
        "<h3> Google Authentication </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92DyxfJhmaeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOGwgNi_rbe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from apache_beam.io.gcp.internal.clients import bigquery\n",
        "table_schema = 'source:STRING, quote:STRING'\n",
        "table_spec = bigquery.TableReference(\n",
        "    projectId=PROJECT,\n",
        "    datasetId='gcp_edgar',\n",
        "    tableId='test_edgar_data')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRCffYk0VA6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "p2 = beam.Pipeline()\n",
        "test_buckt = 'gs://mm_dataflow_bucket/'\n",
        "lines = (\n",
        "     p2\n",
        "     | beam.Create([\n",
        "            {'source': 'Mahatma Gandhi', 'quote': 'My life is my message.'},\n",
        "            {'source': 'Yoda', 'quote': \"Do, or do not. There is no 'try'.\"},\n",
        "        ])\n",
        "     #| 'Filter perennials' >> beam.Filter(\n",
        "     #     lambda row: len(row.split(',')) > 3)\n",
        "     | 'sending to putput' >> beam.Map(print)\n",
        "     #| beam.io.WriteToText('{}{}'.format(test_buckt, 'cutCreate1'))\n",
        "     \n",
        ")\n",
        "p2.run()\n",
        "# visualize output\n",
        "#!({'head -n 20 data/cutCreate1-00000-of-00001'})\n",
        "\n",
        "# check tis link fo rwriting to gcs https://colab.research.google.com/notebooks/io.ipynb#scrollTo=0ENMqxq25szn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86G9mdxqfyOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d77LxRXtQ5f",
        "colab_type": "text"
      },
      "source": [
        "<h3> Edgar MasterIDX URL generation Pipeline </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s3IO_V5tT2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quarters = ['QTR1', 'QTR2', 'QTR3', 'QTR4']\n",
        "full_dir = \"https://www.sec.gov/Archives/edgar/full-index/{year}/{QUARTER}/\"\n",
        "def get_edgar_urls(years:list) :\n",
        "    print('fetching master.idx for year {}'.format(years))\n",
        "    idx_directories = [full_dir.format(year=year, QUARTER=qtr) for year in years for qtr in quarters]\n",
        "    return ['{}'.format(edgar_dir) for edgar_dir in idx_directories]\n",
        "output_bucket =  \"gs://mm_dataflow_bucket/outputs\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTtpHj7VwSTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "from pprint import pprint\n",
        "quarters = ['QTR1', 'QTR2', 'QTR3', 'QTR4']\n",
        "full_dir = \"https://www.sec.gov/Archives/edgar/full-index/{year}/{QUARTER}/\"\n",
        "\n",
        "\n",
        "# Using Beautiful soup\n",
        "import re, requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def processUrl(url):\n",
        "  if 'master.idx' in url:\n",
        "    return url\n",
        "\n",
        "def crawl(base_page):\n",
        "  req=requests.get(base_page)\n",
        "  good_ones = []\n",
        "  if req.status_code==200:\n",
        "      html=BeautifulSoup(req.text,'html.parser')\n",
        "      pages=html.find_all('a')\n",
        "      for page in pages:\n",
        "          url=page.get('href')\n",
        "          res = processUrl(url)\n",
        "          if res:\n",
        "            full_url = '{}{}'.format(base_page, res)\n",
        "            print('Appending..:{}'.format(full_url))\n",
        "            good_ones.append(full_url)\n",
        "      return good_ones\n",
        "\n",
        "def generate_master_urls(all_url):\n",
        "    res = map(lambda u: crawl(u), all_url)\n",
        "    pprint(res)\n",
        "    from itertools import chain\n",
        "    unpacked = chain(*res)\n",
        "    return list(unpacked)\n",
        "\n",
        "def generate_edgar_urls_for_year(year):\n",
        "    test_urls = ['https://www.sec.gov/Archives/edgar/full-index/{}/QTR1/',\n",
        "             'https://www.sec.gov/Archives/edgar/full-index/{}/QTR2/',\n",
        "             'https://www.sec.gov/Archives/edgar/full-index/{}/QTR3/',\n",
        "             'https://www.sec.gov/Archives/edgar/full-index/{}/QTR4/']\n",
        "    urls = map(lambda b_url: b_url.format(year), test_urls)\n",
        "    return generate_master_urls(urls)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832Xtm54z34K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_edgar_urls_for_year('2019')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQAyKp7Ox8kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from past.builtins import unicode\n",
        "class FileExtractingDoFn(beam.DoFn):\n",
        "  \"\"\"Parse each line of input text into words.\"\"\"\n",
        "\n",
        "  def read_file(self, url):\n",
        "    with requests.get(url, stream=True) as r:\n",
        "      r.raise_for_status()\n",
        "      lines = []\n",
        "      print('Writing to:{}'.format(local_filename))\n",
        "      with open(local_filename, 'wb') as f:\n",
        "          for chunk in r.iter_content(chunk_size=8192):\n",
        "              if chunk: # filter out keep-alive new chunks\n",
        "                  f.write(chunk)\n",
        "\n",
        "  def crawl(self, base_page):\n",
        "    req=requests.get(base_page)\n",
        "    good_ones = []\n",
        "    if req.status_code==200:\n",
        "        html=BeautifulSoup(req.text,'html.parser')\n",
        "        pages=html.find_all('a')\n",
        "        for page in pages:\n",
        "            url=page.get('href')\n",
        "            res = processUrl(url)\n",
        "            if res:\n",
        "              full_url = '{}{}'.format(base_page, res)\n",
        "              print('Appending..:{}'.format(full_url))\n",
        "              good_ones.append(full_url)\n",
        "        return good_ones\n",
        "  def processUrl(self, url):\n",
        "    if 'master.idx' in url:\n",
        "      return url\n",
        "\n",
        "  def __init__(self):\n",
        "     # TODO(BEAM-6158): Revert the workaround once we can pickle super() on py3.\n",
        "    # super(WordExtractingDoFn, self).__init__()\n",
        "    beam.DoFn.__init__(self)\n",
        "  \n",
        "  def process(self, element):\n",
        "    \"\"\"Returns an iterator over the words of this element.\n",
        "    The element is a line of text.  If the line is blank, note that, too.\n",
        "    Args:\n",
        "      element: the element being processed\n",
        "    Returns:\n",
        "      The processed element.\n",
        "    \"\"\"\n",
        "    print('Processing {}'.format(element))\n",
        "    return self.crawl(element)\n",
        "\n",
        "p3 = beam.Pipeline()\n",
        "test_buckt = 'gs://mm_dataflow_bucket/'\n",
        "lines = (\n",
        "     p3\n",
        "     | 'generate edgar url' >>beam.Create(generate_edgar_urls_for_year('2019'))\n",
        "     \n",
        "     | 'extract master idx files' >> (beam.ParDo(FileExtractingDoFn())\n",
        "                                          .with_output_types(unicode)) \n",
        "     | 'sending to putput' >> beam.Map(print)\n",
        "     #beam.io.WriteToText('{}{}'.format('data/', 'cutCreate1'))\n",
        ")\n",
        "p3.run()\n",
        "# visualize output\n",
        "#!({'head -n 20 data/cutCreate1-00000-of-00001'})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-yoa7ur29BO",
        "colab_type": "text"
      },
      "source": [
        "<h2> Test Pipeline that reads and parse Remote Filings remotely  </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zkQp0eczthS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.io.textio import ReadAllFromText\n",
        "import urllib\n",
        "from collections import defaultdict\n",
        "from datetime import date, datetime\n",
        "from itertools import groupby\n",
        "p4 = beam.Pipeline()\n",
        "test_bucket = 'gs://mm_dataflow_bucket/'\n",
        "form_type = '13F-HR'\n",
        "filename = '{}_{}'.format(form_type, datetime.now().strftime('%Y$m%d-%H%M'))\n",
        "\n",
        "\n",
        "class ReadRemote(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    print('REadRemote processing///{}'.format(element))\n",
        "    data = urllib.request.urlopen(element) # it's a file like object and works just like a file\n",
        "    return [line for line in data]\n",
        "\n",
        "class ParseForm13F(beam.DoFn):\n",
        "\n",
        "  def open_url_content(self, file_path):\n",
        "    import requests\n",
        "    print('Attepmting to open:{}'.format(file_path))\n",
        "    return requests.get(file_path)\n",
        "\n",
        "  def get_cusips(self, content):\n",
        "    data = content.text\n",
        "    data = data.replace('\\n', '')\n",
        "    subset = data[data.rfind('<XML>') + 5: data.rfind(\"</XML>\")]\n",
        "    from xml.etree import ElementTree\n",
        "    tree = ElementTree.ElementTree(ElementTree.fromstring(subset))\n",
        "    root = tree.getroot()\n",
        "    all_dt =  [child.text for infoTable in root.getchildren() for child in infoTable.getchildren()\n",
        "            if 'cusip' in child.tag]\n",
        "    return all_dt\n",
        "\n",
        "  def _group_data(self, lst):\n",
        "    all_dict = defaultdict(list)\n",
        "    if lst:\n",
        "      print('Attempting to group..')\n",
        "      data = sorted(lst, key=lambda x: x)\n",
        "      for k, g in groupby(data, lambda x: x):\n",
        "        grp = len(list(g))\n",
        "        if grp > 1:\n",
        "          print('{} has {}'.format(k, grp))\n",
        "        all_dict[k].append(grp)\n",
        "      \n",
        "  def process(self, element):\n",
        "    try:\n",
        "      file_content = self.open_url_content(element)\n",
        "      all_cusips = self.get_cusips(file_content)\n",
        "      #self._group_data(all_cusips)\n",
        "      #print('Found:{} in Processing {}'.format(len(all_cusips), element))\n",
        "      return all_cusips\n",
        "    except Exception as e:\n",
        "      print('could not fetch data from {}:{}'.format(element, str(e)))\n",
        "      return []\n",
        "\n",
        "import requests\n",
        "def format_string(input_str):\n",
        "  return str(input_str.replace(\"b'\", \"\").replace(\"'\", \"\")).strip()\n",
        "\n",
        "def cusip_to_ticker(cusip):\n",
        "  try:\n",
        "    #print('Attempting to get ticker for {}'.format(cusip))\n",
        "    cusip_url = \"https://us-central1-datascience-projects.cloudfunctions.net/cusip2ticker/{fullCusip}\".format(fullCusip=cusip)\n",
        "    #print('Opening:{}'.format(cusip_url))\n",
        "    req=requests.get(cusip_url).json()\n",
        "    ticker =  req['ticker']\n",
        "    return format_string(ticker)\n",
        "  except Exception as e:\n",
        "    print('Unable to retrieve ticker for {}'.format(cusip))\n",
        "    return ''\n",
        "\n",
        "## BIG QUERY SCHEMA\n",
        "from apache_beam.io.gcp.internal.clients import bigquery\n",
        "edgar_table_schema = 'COB:STRING, CUSIP:STRING, COUNT:INTEGER, TICKER:STRING'\n",
        "edgar_table_spec = bigquery.TableReference(\n",
        "    projectId=PROJECT,\n",
        "    datasetId='gcp_edgar',\n",
        "    tableId='form_13hf_data')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R38BJSiASQqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p4 = beam.Pipeline()\n",
        "lines = (\n",
        "     p4\n",
        "     #| 'generate master url' >>beam.Create(['https://www.sec.gov/Archives/edgar/full-index/2019/QTR1/master.idx'])\n",
        "     | 'Sampling Data' >> beam.Create(['https://www.sec.gov/Archives/edgar/full-index/2019/QTR1/master.idx',\n",
        "                    #'https://www.sec.gov/Archives/edgar/full-index/2019/QTR2/master.idx'\n",
        "                    ])\n",
        "     | 'readFromText' >> beam.ParDo(ReadRemote())\n",
        "     | 'map to Str'   >> beam.Map(lambda line:str(line))\n",
        "     | 'Filter only form 13HF' >> beam.Filter(lambda row: len(row.split('|')) > 4 and form_type in row.split('|')[2])\n",
        "     | 'Generating Proper file path' >> beam.Map(lambda row: '{}/{}'.format('https://www.sec.gov/Archives', row.split('|')[4]))\n",
        "     | 'replacing eol' >> beam.Map(lambda p: p[0:p.find('\\\\n')])\n",
        "     #| 'sampling lines' >> beam.transforms.combiners.Sample.FixedSizeGlobally(10)\n",
        "     #| 'flat Mapping' >> beam.Map(lambda elements: elements[0])\n",
        "     | 'parsing edgar filing' >> beam.ParDo(ParseForm13F())\n",
        "     | 'Combining similar' >> beam.combiners.Count.PerElement()\n",
        "     | 'Groupring' >> beam.MapTuple(lambda word, count: (word, count))\n",
        "     #| 'sampling again' >> beam.transforms.combiners.Sample.FixedSizeGlobally(20)\n",
        "     #| 'Adding Cusip' >> beam.MapTuple(lambda word, count: (word, cusip_to_ticker(word), count))\n",
        "     #| 'Filtering' >> beam.Filter(lambda tpl: tpl[1] > 300)\n",
        "     #| 'Creating BigQuery Data' >> beam.MapTuple(lambda word, ticker, count: dict(COB=date.today().strftime('%Y-%m-%d'), CUSIP=word, TICKER=ticker,COUNT=count))\n",
        "     #| 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
        "     #                                       edgar_table_spec,\n",
        "     #                                       schema=edgar_table_schema,\n",
        "     #                                       write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
        "     #                                       create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
        "     | 'sending to out' >> beam.Map(print)\n",
        "     #| beam.io.WriteToText('{}{}'.format(test_bucket, filename))\n",
        "     #|  beam.io.WriteToText('cutCreate1-00000-of-00001')\n",
        ")\n",
        "p4.run()\n",
        "# visualize output\n",
        "#!({'head -n 20 cutCreate1-00000-of-00001'})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ATv3ybfxJHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!({'head -n 20 data/cutCreate1-00000-of-00001-00000-of-00001'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fqEUAba03AY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6_HcnZeS7Cs",
        "colab_type": "text"
      },
      "source": [
        "<h3>FEtching Ticker from Cusip </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBXob6wwS2YS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZgVnig0V7WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cusips = ['00401C108',\n",
        "         '00404A109',\n",
        "         '004225108',\n",
        "         '004239109',\n",
        "        '00434H108',\n",
        "         'G1151C101',\n",
        "        '00081T108']\n",
        "\n",
        "for c in cusips:\n",
        "  ticker = cusip_to_ticker(c)\n",
        "  print('CUSIP:{}|TICKER:{}'.format(c, ticker))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wsHhmhUbzIf",
        "colab_type": "text"
      },
      "source": [
        "<h3> Other data to join in </h3>\n",
        "\n",
        "\n",
        "1.   Price Targets:GET /stock/{symbol}/price-target (500 messages per symbol)\n",
        "2.   Estimates:https://iexcloud.io/docs/api/#estimates (10000 per symbols)\n",
        "3.   Key Stats:GET /stock/{symbol}/stats/{stat?} day200MovingAvg, peRatio, beta,ytdChangePercent\n",
        "4. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJLLShdcvBer",
        "colab_type": "text"
      },
      "source": [
        "<h3>Price Targets </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3p_1Eg4S2ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def get_iexapi_keys():\n",
        "  with open('gdrive/My Drive/passwords/iexapi.keys') as f:\n",
        "    return f.readlines()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-awAVnVWS2ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "def get_price_targets(symbol):\n",
        "  price_targets_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/price-target?token={token}'.format(symbol=symbol, token=get_iexapi_keys())\n",
        "  return requests.get(price_targets_url).json()\n",
        "\n",
        "def get_analysts_recommendation_trends(symbol):\n",
        "  price_targets_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/recommendation-trends?token={token}'.format(symbol=symbol, token=get_iexapi_keys())\n",
        "  return requests.get(price_targets_url).json()\n",
        "\n",
        "get_analysts_recommendation('VZ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HpZcMCKw3l9",
        "colab_type": "text"
      },
      "source": [
        "<h3> Estimates </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq9oNMDH2SMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_estimates(symbol):\n",
        "  price_targets_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/estimates?token={token}'.format(symbol=symbol, token=get_iexapi_keys())\n",
        "  return requests.get(price_targets_url).json()\n",
        "\n",
        "get_estimates('JNJ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LJOb73j3caf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from apache_beam.transforms.combiners import Sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tkjvJIO2Rk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test read remote file\n",
        "def open_url_content(file_path):\n",
        "  import requests\n",
        "  return requests.get(file_path)\n",
        "\n",
        "def get_cusips(file_content):\n",
        "  data = content.text\n",
        "  data = data.replace('\\n', '')\n",
        "  subset = data[data.rfind('<XML>') + 5: data.rfind(\"</XML>\")]\n",
        "  from xml.etree import ElementTree\n",
        "  tree = ElementTree.ElementTree(ElementTree.fromstring(subset))\n",
        "  root = tree.getroot()\n",
        "  all_dt =  [child.text for infoTable in root.getchildren() for child in infoTable.getchildren()\n",
        "          if 'cusip' in child.tag]\n",
        "  return all_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z23Ih4FISVfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorting a pipeline in python\n",
        "p2 = beam.Pipeline()\n",
        "lines = (\n",
        "     p2\n",
        "     | beam.Create([('Test', 1), ('Another', 5), ('Third', 4)])\n",
        "     | 'Sorting values perennials' >> beam.Filter(\n",
        "          lambda row: len(row.split(',')) > 3)\n",
        "     | 'sending to putput' >> beam.Map(print)\n",
        "     #| beam.io.WriteToText('{}{}'.format(test_buckt, 'cutCreate1'))\n",
        ")\n",
        "p2.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MvlrFeHW6G2",
        "colab_type": "text"
      },
      "source": [
        "<h3> Testing a PIpelien for retrieving shares </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGTxC2XSW_Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas-datareader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKu-xLCtXbiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import BDay\n",
        "import pandas_datareader.data as dr\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "import requests\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0YFMGJye4IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_shares_dataframe():\n",
        "  all_shares = requests.get('https://k1k1xtrm88.execute-api.us-west-2.amazonaws.com/test/query-shares').json()\n",
        "  ds = [d for d in all_shares if d['QTY'] > 1]\n",
        "  return pd.DataFrame.from_dict(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6taKK6DXft8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_latest_price_yahoo(symbol, cob_date):\n",
        "  try:#\n",
        "    print('--latest price for{}'.format(symbol))\n",
        "    start_date = cob_date - BDay(1)\n",
        "    dfy = dr.get_data_yahoo(symbol, start_date, start_date)[['Adj Close']]\n",
        "    dft = dr.get_data_yahoo(symbol, cob_date, cob_date)[['Adj Close']]\n",
        "    dfy['symbol'] = symbol\n",
        "    dft['symbol'] = symbol\n",
        "    print(dfy.shape)\n",
        "    print(dft.shape)\n",
        "\n",
        "    merged = pd.merge(dft, dfy,on='symbol', suffixes=('_t', '_y'),)\n",
        "    merged['diff'] = merged['Adj Close_t'] - merged['Adj Close_y']\n",
        "    print('Merged shap eis:{}'.format(merged.shape))\n",
        "    return merged.iloc[0].to_dict()\n",
        "                                                           \n",
        "    \n",
        "  except Exception as e :\n",
        "    print('Unable to find data for {}'.format(symbol))\n",
        "    return pd.DataFrame.from_dict({'symbol': [symbol], 'Adj Close_t': [0], 'Adj Close_y':[0], 'diff':[0]}).to_dict()\n",
        "\n",
        "def get_prices(symbols):\n",
        "  prices_dfs = (get_latest_price_yahoo(symbol, date.today()) for symbol in symbols)\n",
        "  all_data = pd.concat(prices_dfs)\n",
        "  return all_data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZi0AHSrDdB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FetchPortfolio(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # 1 call retur sharename, ticker, qty, original price\n",
        "    print('REadRemote processing///{}'.format(element))\n",
        "    data = urllib.request.urlopen(element) # it's a file like object and works just like a file\n",
        "    return [line for line in data]\n",
        "\n",
        "class FetchPrices(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # for each ticker return close today, close yesterday dif\n",
        "    print('REadRemote processing///{}'.format(element))\n",
        "    data = urllib.request.urlopen(element) # it's a file like object and works just like a file\n",
        "    return [line for line in data]\n",
        "  \n",
        "class Accumulator(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # for each ticker generate row with all needed data\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONbhFrst4Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_latest_price_yahoo('AAPL', date.today()-BDay(1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecAqH5OFXpqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_shares_df = get_all_shares_dataframe()\n",
        "symbols = all_shares_df['TICKER'].values[0:20]\n",
        "prices_data = get_prices(symbols)\n",
        "pd.merge(all_shares_df, prices_data, left_on='TICKER', right_on='symbol')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXwYpcvXDAO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p2 = beam.Pipeline()\n",
        "lines = (\n",
        "     p2\n",
        "     | beam.Create(['AAPL', 'MSFT', 'AMZN'])\n",
        "     | 'Sorting values perennials' >> beam.Filter(\n",
        "          lambda row: len(row.split(',')) > 3)\n",
        "     | 'sending to putput' >> beam.Map(print)\n",
        "     #| beam.io.WriteToText('{}{}'.format(test_buckt, 'cutCreate1'))\n",
        ")\n",
        "p2.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS1WZVbgXuwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res.diff(axis=1, periods=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwkB-bQVa7cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}